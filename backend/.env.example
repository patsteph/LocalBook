# LocalBook Backend Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# LLM Settings
# =============================================================================
# Provider: ollama (default), openai, or anthropic
LLM_PROVIDER=ollama

# Ollama settings (for local LLM)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral-nemo

# OpenAI settings (optional - if using OpenAI)
# OPENAI_API_KEY=sk-your-key-here

# Anthropic settings (optional - if using Anthropic)
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# =============================================================================
# Embedding Settings
# =============================================================================
# Model for generating document embeddings
# Options: BAAI/bge-small-en-v1.5 (default), all-MiniLM-L6-v2
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# =============================================================================
# Chunking Settings (affects retrieval quality)
# =============================================================================
# Characters per chunk (smaller = more precise, larger = more context)
CHUNK_SIZE=1000

# Overlap between chunks (higher = better continuity)
CHUNK_OVERLAP=200

# Recommended settings by use case:
# - Technical docs:    CHUNK_SIZE=800,  CHUNK_OVERLAP=150
# - Long narratives:   CHUNK_SIZE=1200, CHUNK_OVERLAP=250
# - Mixed content:     CHUNK_SIZE=1000, CHUNK_OVERLAP=200 (default)

# =============================================================================
# Server Settings
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8000
