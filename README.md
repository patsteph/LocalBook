# LocalBook

**Your documents, your AI, your machine.** A private, offline alternative to cloud-based AI assistants.

![LocalBook](https://img.shields.io/badge/Platform-macOS-blue) ![Python](https://img.shields.io/badge/Python-3.10+-green) ![License](https://img.shields.io/badge/License-MIT-yellow)

## What is LocalBook?

LocalBook lets you **chat with your documents** using AI â€” completely offline and private. Upload PDFs, Word docs, web pages, or YouTube videos, then ask questions and get answers with exact citations.

- ğŸ”’ **100% Private** â€” Everything runs on your Mac
- ğŸ“š **Your Documents** â€” AI answers from YOUR files with citations
- ï¿½ **Knowledge Constellation** â€” 3D visualization of concepts across documents
- ğŸ§  **Memory System** â€” AI remembers your preferences and past conversations
- ğŸ™ï¸ **Podcast Generator** â€” Turn documents into audio discussions
- ï¿½ **Auto-Updates** â€” Check for and pull updates from GitHub

---

## Quick Start

```bash
# Clone and build (~15-20 min first time)
git clone https://github.com/patsteph/LocalBook.git
cd LocalBook
./build.sh

# Install
cp -r LocalBook.app /Applications/
```

The build script installs everything: Homebrew, Python, Node.js, Rust, Ollama, AI models (~10GB), and all dependencies.

---

## Requirements

### System
- **macOS** (required for audio generation)
- **16GB+ RAM** recommended (8GB minimum)
- **~15GB storage** for models and app
- **Apple Silicon** recommended (Intel works but slower)

### System Dependencies
| Dependency | Purpose | Install |
|------------|---------|--------|
| **Ollama** | Local LLM inference | `brew install ollama` |
| **ffmpeg** | Audio/video transcription | `brew install ffmpeg` |
| **Python 3.10+** | Backend | `brew install python@3.11` |
| **Node.js 18+** | Frontend build | `brew install node` |
| **git** | Updates | Pre-installed on macOS |

> The `build.sh` script installs all of these automatically.

### AI Models (pulled by build script)
```bash
ollama pull mistral-nemo:12b-instruct-2407-q4_K_M  # Main model (~7GB)
ollama pull phi4-mini                               # Fast model (~2GB)
```

---

## Features

### Core Features
| Feature | Description |
|---------|-------------|
| ğŸ’¬ **Chat** | Ask questions, get answers with citations |
| ğŸ“„ **Multi-format** | PDF, Word, PowerPoint, Excel, web pages, YouTube |
| ğŸ” **Web Search** | Optionally supplement with real-time web results |
| ğŸ“… **Timeline** | Auto-extract and visualize dates/events |

### New in v0.1.0
| Feature | Description |
|---------|-------------|
| ğŸŒŒ **Constellation** | 3D knowledge graph showing concept relationships |
| ğŸ§  **Memory** | AI remembers facts about you across sessions |
| ğŸ¨ **Notebook Colors** | Color-code notebooks for organization |
| ğŸ”„ **Updates** | Check for updates in Settings â†’ Updates |

---

## Development

```bash
# Run in development mode with hot-reload
./start.sh
```

### Project Structure
```
LocalBook/
â”œâ”€â”€ backend/           # Python FastAPI backend
â”‚   â”œâ”€â”€ api/          # API endpoints
â”‚   â”œâ”€â”€ services/     # Business logic (RAG, memory, knowledge graph)
â”‚   â””â”€â”€ storage/      # Database and vector storage
â”œâ”€â”€ src/              # React frontend
â”œâ”€â”€ src-tauri/        # Tauri desktop app
â””â”€â”€ data/             # Local data (gitignored)
```

### API Docs
When running: http://localhost:8000/docs

---

## Configuration

### Settings (in-app)
- **API Keys**: Brave Search, OpenAI, Anthropic
- **Memory**: View/manage AI memory
- **Updates**: Check for new versions

### Environment (`backend/.env`)
```bash
OLLAMA_MODEL=mistral-nemo:12b-instruct-2407-q4_K_M
OLLAMA_FAST_MODEL=phi4-mini
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

---

## Data Storage

All data stored locally in `data/` (gitignored):
- `data/uploads/` â€” Your documents
- `data/lancedb/` â€” Vector embeddings
- `data/memory/` â€” AI memory (persists across updates)
- `data/audio/` â€” Generated podcasts

---

## Troubleshooting

### Ollama Issues
```bash
curl http://localhost:11434/api/tags  # Check if running
ollama serve                           # Start if not
ollama list                            # Verify models
```

### Clean Rebuild
```bash
rm -rf src-tauri/resources/backend/ src-tauri/target/ node_modules/ backend/.venv/
./build.sh
```

### Memory Not Working
Restart the backend after updating. Memory is extracted from chat conversations automatically.

---

## License

MIT â€” See LICENSE file.

---

## Acknowledgments

Inspired by Google's NotebookLM, built for privacy-conscious users who want local document AI.
