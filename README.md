# LocalBook

**Your documents, your AI, your machine.** A private, offline alternative to cloud-based AI assistants.

![LocalBook](https://img.shields.io/badge/Platform-macOS-blue) ![Python](https://img.shields.io/badge/Python-3.10+-green) ![License](https://img.shields.io/badge/License-MIT-yellow)

---

## What is LocalBook?

Chat with your documents using AI â€” completely offline and private. Upload PDFs, Word docs, web pages, or YouTube videos, then ask questions and get answers with exact citations.

- ğŸ”’ **100% Private** â€” Everything runs locally on your Mac
- ğŸ“š **Cited Answers** â€” AI answers from YOUR files with source citations
- ğŸŒŒ **Knowledge Constellation** â€” 3D visualization of concepts across documents
- ğŸ§  **Memory System** â€” AI remembers your preferences across sessions
- ğŸ™ï¸ **Podcast Generator** â€” Turn documents into audio discussions

---

## Requirements

| Requirement | Details |
|-------------|---------|
| **macOS** | Required (Apple Silicon recommended, Intel supported) |
| **RAM** | 16GB+ recommended (8GB minimum) |
| **Storage** | ~15GB for models and app |
| **Ollama** | Local LLM runtime ([ollama.ai](https://ollama.ai)) |

### System Dependencies

The build script installs these automatically, or install manually:

```bash
brew install ollama ffmpeg tesseract python@3.11 node
```

---

## Quick Start

```bash
git clone https://github.com/patsteph/LocalBook.git
cd LocalBook
./build.sh
cp -r LocalBook.app /Applications/
```

Build takes ~15-20 minutes on first run (downloads models, installs dependencies).

### âš¡ Speed Up First Launch

Pre-download AI models before building to save time on first startup:

```bash
# Required models (~6GB total)
ollama pull olmo-3:7b-instruct      # Main reasoning model
ollama pull phi4-mini               # Fast model
ollama pull snowflake-arctic-embed2 # Embeddings (1024 dims)
```

---

## Features

| Feature | Description |
|---------|-------------|
| ğŸ’¬ **Chat** | Ask questions, get answers with citations |
| ğŸ“„ **Multi-format** | PDF, Word, PowerPoint, Excel, EPUB, Jupyter, Images (OCR), YouTube |
| ğŸ” **Web Search** | Supplement answers with real-time web results |
| ğŸ“… **Timeline** | Auto-extract and visualize dates/events |
| ğŸŒŒ **Constellation** | 3D knowledge graph with clustering |
| ğŸ§  **Memory** | AI remembers facts about you across sessions |
| ğŸ™ï¸ **Podcasts** | Generate audio discussions from documents |

### What's New in v0.6

| Feature | Description |
|---------|-------------|
| ğŸ¯ **Query Orchestrator** | Complex queries auto-decompose into sub-questions |
| ğŸ“– **Parent Document Retrieval** | Retrieves surrounding context for better answers |
| ğŸ•¸ï¸ **Entity Graph** | Tracks people, metrics, and relationships |
| ğŸ”„ **Migration Manager** | Seamless upgrades with progress notifications |
| â„ï¸ **Snowflake Embeddings** | Upgraded to 1024-dim frontier embeddings |
| âš¡ **Phi-4 Mini** | Faster responses with Microsoft's latest small model |

### What's New in v0.5

| Feature | Description |
|---------|-------------|
| ğŸ¯ **Adaptive RAG** | Two-tier model routing (fast vs deep thinking) |
| ğŸ”€ **Hybrid Search** | Vector + BM25 keyword search combined |
| ğŸ“Š **FlashRank Reranking** | Cross-encoder reranking for better retrieval |
| âœ¨ **Cleaner Answers** | Improved prompt engineering, no artifacts |

---

## Upgrading

### From v0.5
Automatic incremental upgrade. Just replace the app and restart.

### From v0.2/v0.3
Automatic migration on first launch. Documents will be re-indexed with new embeddings.

### From v0.1.x
Data was stored inside the app bundle. Run this **before** replacing the app:
```bash
curl -sL https://raw.githubusercontent.com/patsteph/LocalBook/master/migrate_data.sh | bash
```

---

## Configuration

### In-App Settings
- **API Keys** â€” Brave Search, OpenAI, Anthropic (optional)
- **Memory** â€” View/manage what AI remembers
- **Updates** â€” Check for new versions

### Environment (`backend/.env`)
```bash
OLLAMA_MODEL=olmo-3:7b-instruct       # Main reasoning (64K context)
OLLAMA_FAST_MODEL=phi4-mini           # Fast responses
EMBEDDING_MODEL=snowflake-arctic-embed2  # 1024-dim embeddings
```

---

## Data Storage

All data stored in `~/Library/Application Support/LocalBook/`:

| Directory | Contents |
|-----------|----------|
| `uploads/` | Your documents |
| `lancedb/` | Vector embeddings |
| `memory/` | AI memory (persists across updates) |
| `audio/` | Generated podcasts |
| `backups/` | Pre-migration backups |

---

## Development

```bash
./start.sh  # Run with hot-reload
```

API docs available at http://localhost:8000/docs when running.

### Project Structure
```
LocalBook/
â”œâ”€â”€ backend/           # Python FastAPI
â”‚   â”œâ”€â”€ api/          # REST endpoints
â”‚   â”œâ”€â”€ services/     # RAG, memory, knowledge graph
â”‚   â””â”€â”€ storage/      # LanceDB, file storage
â”œâ”€â”€ src/              # React frontend
â””â”€â”€ src-tauri/        # Tauri desktop wrapper
```

---

## Troubleshooting

### Ollama Not Running
```bash
ollama serve          # Start Ollama
ollama list           # Verify models installed
```

### Models Missing
```bash
ollama pull olmo-3:7b-instruct
ollama pull phi4-mini
ollama pull snowflake-arctic-embed2
```

### Clean Rebuild
```bash
rm -rf src-tauri/resources/backend/ src-tauri/target/ node_modules/ backend/.venv/
./build.sh
```

---

## License

MIT â€” See LICENSE file.

---

Built for privacy-conscious users who want local document AI. Inspired by Google's NotebookLM.
